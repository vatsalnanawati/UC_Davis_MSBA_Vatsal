{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Ghj6miADOzPi"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vatsalnanawati/UC_Davis_MSBA_Vatsal/blob/main/%20Google_Collab/Machine_Learning_HW5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. [Word2Vec] Explore Word Embeddings and Cosine Similarity"
      ],
      "metadata": {
        "id": "Ghj6miADOzPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    # Compute dot product\n",
        "    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
        "\n",
        "    # Compute magnitude of each vector\n",
        "    magnitude_vec1 = math.sqrt(sum(a**2 for a in vec1))\n",
        "    magnitude_vec2 = math.sqrt(sum(b**2 for b in vec2))\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    if magnitude_vec1 == 0 or magnitude_vec2 == 0:\n",
        "        return 0  # Avoid division by zero\n",
        "    return dot_product / (magnitude_vec1 * magnitude_vec2)\n",
        "\n",
        "# Test cases\n",
        "vec_a = [1, 2, 3]\n",
        "vec_b = [1, 2, 3]  # Identical vectors, similarity should be 1\n",
        "vec_c = [1, 0, 0]  # Orthogonal to vec_d\n",
        "vec_d = [0, 1, 0]  # Orthogonal to vec_c, similarity should be 0\n",
        "\n",
        "print(\"Cosine Similarity (vec_a, vec_b):\", cosine_similarity(vec_a, vec_b))  # Expected: 1\n",
        "print(\"Cosine Similarity (vec_c, vec_d):\", cosine_similarity(vec_c, vec_d))  # Expected: 0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Oe1c-baOyXK",
        "outputId": "224965e8-8a62-4e21-e7ce-b06d56a3415f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity (vec_a, vec_b): 1.0\n",
            "Cosine Similarity (vec_c, vec_d): 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "model = api.load(\"word2vec-google-news-300\")  # Downloads automatically\n",
        "print(\"Loaded Google Word2Vec model from gensim downloader!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4cqy5w0QYNo",
        "outputId": "3fc01e22-ab41-4af5-8d5d-301241634e0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "Loaded Google Word2Vec model from gensim downloader!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0OpqquvOtBe",
        "outputId": "65f03e07-4648-4ed2-e2be-fb91137acd02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity (king, queen): 0.6511\n",
            "Cosine Similarity (happy, joyful): 0.4238\n",
            "Cosine Similarity (cat, dog): 0.7609\n",
            "Cosine Similarity (computer, banana): 0.0908\n",
            "Cosine Similarity (love, hate): 0.6004\n",
            "Cosine Similarity (hot, cold): 0.4602\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "\n",
        "# Selected word pairs\n",
        "similar_pairs = [(\"king\", \"queen\"), (\"happy\", \"joyful\"), (\"cat\", \"dog\")]\n",
        "dissimilar_pairs = [(\"computer\", \"banana\"), (\"love\", \"hate\"), (\"hot\", \"cold\")]\n",
        "\n",
        "# Compute cosine similarity for each pair\n",
        "for w1, w2 in similar_pairs + dissimilar_pairs:\n",
        "    similarity = cosine_similarity(model[w1], model[w2])\n",
        "    print(f\"Cosine Similarity ({w1}, {w2}): {similarity:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis:\n",
        "\n",
        "Key Observations:\n",
        "* (king, queen) → 0.6511 ✅ Expected high similarity as both are related to royalty and appear in similar contexts.\n",
        "* (happy, joyful) → 0.4238 ⚠️ Lower than expected (anticipated ~0.7-0.8).\n",
        "Both express positive emotions, but the model may distinguish them based on usage patterns or emotional intensity.\n",
        "* (cat, dog) → 0.7609✅ High similarity as expected.\n",
        "Both are common pets and frequently appear together in similar contexts.\n",
        "* (computer, banana) → 0.0908 ✅ Very low similarity, as expected.\n",
        "These words belong to completely different semantic categories (technology vs. fruit).\n",
        "* (love, hate) → 0.6004⚠️ Higher than expected; expected lower similarity (~0.2-0.3). Antonyms but frequently appear in similar sentence structures (e.g., \"I love this movie\" vs. \"I hate this movie\"), causing overlap in embeddings.\n",
        "* (hot, cold) → 0.4602⚠️ Slightly high for antonyms; expected a lower similarity score.These words often appear in similar discussions (e.g., \"The weather is hot\" vs. \"The weather is cold\"), highlighting how word embeddings focus on context rather than strict oppositeness.\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "* Synonyms may have lower similarity than expected due to how embeddings differentiate contextual meanings.\n",
        "* Antonyms can have higher similarity if they frequently appear in similar contexts (e.g., love/hate, hot/cold).\n",
        "* Context matters more than strict meaning in word embeddings, influencing similarity scores.\n",
        "* Distributional semantics captures co-occurrence but may not align perfectly with human intuition regarding word relationships.\n",
        "\n",
        "### Conclusion:\n",
        "Cosine similarity provides useful insights into word relationships but has limitations when dealing with synonyms and antonyms due to its reliance on context rather than strict semantic meaning."
      ],
      "metadata": {
        "id": "Yp2TcbDBj5Kz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2  Sequence Prediction\n"
      ],
      "metadata": {
        "id": "__LVUtryNo1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Two lists: X is the current Fibonacci number, y is the next one\n",
        "X = np.array([  0,   1,   1,   2,   3,   5,    8,   13,   21,   34], dtype=np.float32)\n",
        "y = np.array([  1,   1,   2,   3,   5,   8,   13,   21,   34,   55], dtype=np.float32)\n",
        "\n",
        "print(\"X =\", X)\n",
        "print(\"y =\", y)\n"
      ],
      "metadata": {
        "id": "REHv6hXkNrM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c88340a-3088-46a2-8081-86e354aea832"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X = [ 0.  1.  1.  2.  3.  5.  8. 13. 21. 34.]\n",
            "y = [ 1.  1.  2.  3.  5.  8. 13. 21. 34. 55.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to tensors and reshape for RNN\n",
        "# RNN expects shape (batch_size, seq_length, input_size)\n",
        "x_tensor = torch.tensor(X).view(-1, 1, 1)  # shape => (13, 1, 1)\n",
        "y_tensor = torch.tensor(y).view(-1, 1)     # shape => (13, 1)\n",
        "\n",
        "print(\"x_tensor shape:\", x_tensor.shape)\n",
        "print(\"y_tensor shape:\", y_tensor.shape)\n"
      ],
      "metadata": {
        "id": "eTQSb3VjOe5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bd2e02b-08ca-4cf3-b48f-ba47ab2b8009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_tensor shape: torch.Size([10, 1, 1])\n",
            "y_tensor shape: torch.Size([10, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=10, output_size=1):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        rnn_out, hidden = self.rnn(x, hidden)   # => shape [batch, seq_length, hidden_size]\n",
        "        # Only the last timestep => rnn_out[:, -1, :] is shape (batch, hidden_size)\n",
        "        out = self.fc(rnn_out[:, -1, :])        # => shape (batch, 1)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # RNN has 1 layer => shape (num_layers=1, batch_size, hidden_size)\n",
        "        return torch.zeros(1, batch_size, self.hidden_size)\n"
      ],
      "metadata": {
        "id": "9baIZx6kOsyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_rnn(model, x_tensor, y_tensor, epochs=10000, lr=0.01):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        hidden = model.init_hidden(x_tensor.size(0))  # shape (1, batch=13, 10)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, _ = model(x_tensor, hidden)\n",
        "\n",
        "        loss = criterion(outputs, y_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (epoch + 1) % 500 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss={loss.item():.6f}\")\n",
        "\n",
        "# Instantiate and train\n",
        "model = SimpleRNN(input_size=1, hidden_size=10, output_size=1)\n",
        "train_rnn(model, x_tensor, y_tensor, epochs=10000, lr=0.01)\n"
      ],
      "metadata": {
        "id": "AUDu4JpqO9lD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50b47e69-f4e8-45fd-a063-a486e6737096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 500/10000, Loss=38.408428\n",
            "Epoch 1000/10000, Loss=3.010887\n",
            "Epoch 1500/10000, Loss=0.156672\n",
            "Epoch 2000/10000, Loss=0.091861\n",
            "Epoch 2500/10000, Loss=0.087659\n",
            "Epoch 3000/10000, Loss=0.086636\n",
            "Epoch 3500/10000, Loss=0.085316\n",
            "Epoch 4000/10000, Loss=0.084567\n",
            "Epoch 4500/10000, Loss=0.084435\n",
            "Epoch 5000/10000, Loss=0.082856\n",
            "Epoch 5500/10000, Loss=0.082019\n",
            "Epoch 6000/10000, Loss=0.081493\n",
            "Epoch 6500/10000, Loss=0.078831\n",
            "Epoch 7000/10000, Loss=0.076156\n",
            "Epoch 7500/10000, Loss=0.126197\n",
            "Epoch 8000/10000, Loss=0.064538\n",
            "Epoch 8500/10000, Loss=0.056914\n",
            "Epoch 9000/10000, Loss=0.051964\n",
            "Epoch 9500/10000, Loss=0.051520\n",
            "Epoch 10000/10000, Loss=0.051514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's predict the next number after the last input in X (which is 144)\n",
        "def predict_next(model, last_input):\n",
        "    \"\"\"\n",
        "    Given a raw number 'last_input', shape it for the RNN => (1, 1, 1),\n",
        "    do a forward pass, and return the single predicted float.\n",
        "    \"\"\"\n",
        "    x_test = torch.tensor([[[last_input]]], dtype=torch.float32)\n",
        "    hidden = model.init_hidden(batch_size=1)\n",
        "    with torch.no_grad():\n",
        "        out, _ = model(x_test, hidden)\n",
        "    return out.item()\n",
        "\n",
        "# The last item in X is 34\n",
        "predicted = predict_next(model, 34.0)\n",
        "print(f\"\\nPredicted value after 34.0: {predicted:.4f}\")\n",
        "print(\"Expected next in Fibonacci is 55.0\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiFLTypAQa1n",
        "outputId": "4c2d1fec-ba65-433a-ba85-d75544382361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicted value after 34.0: 54.9995\n",
            "Expected next in Fibonacci is 55.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sequence(model, start_val, num_steps=2):\n",
        "    \"\"\"\n",
        "    Predict the next `num_steps` numbers in the sequence, starting from 'start_val'.\n",
        "    After each prediction, we feed that prediction back in as the 'last_input'.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    current_val = start_val\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        x_test = torch.tensor([[[current_val]]], dtype=torch.float32)\n",
        "        hidden = model.init_hidden(batch_size=1)\n",
        "        with torch.no_grad():\n",
        "            out, _ = model(x_test, hidden)\n",
        "        next_val = out.item()\n",
        "        predictions.append(next_val)\n",
        "        current_val = next_val  # feed the new value back in\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "next_two = predict_sequence(model, start_val=55.0, num_steps=2)\n",
        "print(\"\\nNext 2 predictions after 55.0:\", next_two)\n",
        "print(\"Expected Fibonacci sequence after 55.0 is roughly [89.0, 144.0, ...]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-03As62z1Bw",
        "outputId": "2ae384ce-9632-4e86-8de2-a3e4ab65bbc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Next 2 predictions after 55.0: [55.34815979003906, 55.35304260253906]\n",
            "Expected Fibonacci sequence after 55.0 is roughly [89.0, 144.0, ...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code constructs a single-step RNN that learns to map one Fibonacci number to the next by training on pairs like and so on. The network is good at predicting the immediate successor because it has learned the small, one-step increments in the data. However, when we try to produce additional future forecasts by feeding its own output back in, the model's little inaccuracies add up—so after accurately projecting 55, it does not \"jump\" up to 89 or 144.\n",
        "It does not \"jump\" to 89 or 144. During training, the network only saw single-value inputs and outputs, therefore it did not learn the two-value Fibonacci dependency. To deal with this, we either train with a window of two inputs or switch to a more robust sequence architecture like an LSTM/GRU, both of which are better at capturing the multi-step growth inherent in Fibonacci.\n"
      ],
      "metadata": {
        "id": "zaolFsly-vmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Implementing an RNN from Scratch for Sentiment Analysis Using the IMDB Dataset"
      ],
      "metadata": {
        "id": "WHNYH_8H_Yz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnERm5f7ZFsU",
        "outputId": "b9b5051f-e25e-4fe0-f45b-7cb0a6819da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "# 1) Load the IMDB dataset from Hugging Face\n",
        "imdb_raw = load_dataset(\"imdb\")\n",
        "# 2) Create a validation split from the training set\n",
        "# We'll take 90% for actual training, 10% for validation\n",
        "imdb_split = imdb_raw[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
        "train_data = imdb_split[\"train\"]\n",
        "val_data   = imdb_split[\"test\"]\n",
        "test_data  = imdb_raw[\"test\"]\n",
        "\n",
        "# 3) Basic Text Preprocessing & Tokenization\n",
        "\n",
        "def simple_tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "# We'll build a vocabulary of the most common words, then convert each word to an integer ID\n",
        "from collections import Counter\n",
        "\n",
        "# Collect word frequencies from the training set only\n",
        "word_counter = Counter()\n",
        "for example in train_data:\n",
        "    tokens = simple_tokenize(example[\"text\"])\n",
        "    word_counter.update(tokens)\n",
        "\n",
        "vocab_size = 20000\n",
        "most_common = word_counter.most_common(vocab_size - 2)\n",
        "word2idx = {\"<unk>\": 0, \"<pad>\": 1}\n",
        "idx = 2\n",
        "for word, _ in most_common:\n",
        "    word2idx[word] = idx\n",
        "    idx += 1\n",
        "\n",
        "def text_to_sequence(text, max_len=200):\n",
        "    \"\"\"\n",
        "    Convert text to a list of integer IDs, pad or truncate to 'max_len'.\n",
        "    \"\"\"\n",
        "    tokens = simple_tokenize(text)\n",
        "    seq = []\n",
        "    for token in tokens:\n",
        "        if token in word2idx:\n",
        "            seq.append(word2idx[token])\n",
        "        else:\n",
        "            seq.append(word2idx[\"<unk>\"])\n",
        "    # Pad or truncate\n",
        "    seq = seq[:max_len]\n",
        "    seq += [word2idx[\"<pad>\"]] * (max_len - len(seq))\n",
        "    return seq\n",
        "\n",
        "# 4) Convert IMDB Examples to Tensor Format\n",
        "\n",
        "def encode_example(example, max_len=200):\n",
        "    text_ids = text_to_sequence(example[\"text\"], max_len=max_len)\n",
        "    label    = example[\"label\"]  # 0=negative, 1=positive\n",
        "    return {\"input_ids\": text_ids, \"label\": label}\n",
        "\n",
        "train_encoded = [encode_example(ex) for ex in train_data]\n",
        "val_encoded   = [encode_example(ex) for ex in val_data]\n",
        "test_encoded  = [encode_example(ex) for ex in test_data]\n"
      ],
      "metadata": {
        "id": "8NWFcxbFYeT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class MyManualRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_size, num_classes=2):\n",
        "        super(MyManualRNN, self).__init__()\n",
        "\n",
        "        # 1) Embedding layer\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim, padding_idx=0)\n",
        "\n",
        "        # 2) Manual RNN parameters: W_ih, W_hh, b_h\n",
        "        self.hidden_size = hidden_size\n",
        "        self.W_ih = nn.Parameter(torch.Tensor(hidden_size, embed_dim))\n",
        "        self.W_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_h  = nn.Parameter(torch.Tensor(hidden_size))\n",
        "        # 3) Output layer: final hidden → class logits\n",
        "        self.fc_out = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "        # Initialize parameters (Xavier or uniform)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"Initialize RNN weights and biases.\"\"\"\n",
        "        nn.init.xavier_uniform_(self.W_ih)\n",
        "        nn.init.xavier_uniform_(self.W_hh)\n",
        "        nn.init.zeros_(self.b_h)\n",
        "        # Also init the fc_out similarly\n",
        "        nn.init.xavier_uniform_(self.fc_out.weight)\n",
        "        nn.init.zeros_(self.fc_out.bias)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: shape (batch_size, seq_len)\n",
        "        Returns logits: shape (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        # Embedding => (batch_size, seq_len, embed_dim)\n",
        "        embedded = self.embedding(input_ids)\n",
        "\n",
        "        # Initialize hidden state to zeros => (batch_size, hidden_size)\n",
        "        batch_size = embedded.size(0)\n",
        "        h_t = embedded.new_zeros(batch_size, self.hidden_size)\n",
        "\n",
        "        # For each timestep in seq_len:\n",
        "        seq_len = embedded.size(1)\n",
        "        for t in range(seq_len):\n",
        "            x_t = embedded[:, t, :]  # (batch_size, embed_dim)\n",
        "            # h_t = tanh(W_ih x_t^T + W_hh h_(t-1)^T + b_h)\n",
        "            h_t = torch.tanh(\n",
        "                F.linear(x_t, self.W_ih) +  # = x_t @ W_ih^T\n",
        "                F.linear(h_t, self.W_hh) + # = h_(t-1) @ W_hh^T\n",
        "                self.b_h\n",
        "            )\n",
        "\n",
        "        # Final hidden => output layer\n",
        "        logits = self.fc_out(h_t)  # (batch_size, num_classes)\n",
        "        return logits\n",
        "\n",
        "    def print_model_summary(self):\n",
        "        \"\"\"Print a summary of the model and parameter counts.\"\"\"\n",
        "        total_params = 0\n",
        "        for name, param in self.named_parameters():\n",
        "            num_params = param.numel()\n",
        "            total_params += num_params\n",
        "            print(f\"{name}: shape={tuple(param.shape)}, params={num_params}\")\n",
        "        print(f\"Total trainable parameters: {total_params}\")\n"
      ],
      "metadata": {
        "id": "8-_xhHdtZPyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def manual_backprop(model, input_ids, labels, learning_rate=1e-3):\n",
        "    \"\"\"\n",
        "    compute gradients manually for a single batch\n",
        "\n",
        "    \"\"\"\n",
        "    batch_size, seq_len = input_ids.shape\n",
        "    hidden_size = model.hidden_size\n",
        "\n",
        "    embedded = model.embedding(input_ids)\n",
        "    # We'll store each h_t for time steps, plus each x_t\n",
        "    hs = [ embedded.new_zeros(batch_size, hidden_size) ]  # h_0 = zeros\n",
        "    xts = []\n",
        "\n",
        "    # Forward through time manually:\n",
        "    for t in range(seq_len):\n",
        "        x_t = embedded[:, t, :]\n",
        "        xts.append(x_t)\n",
        "        # h_t = tanh(W_ih x_t + W_hh h_(t-1) + b_h)\n",
        "        # We'll store the pre-activation as well for derivative wrt. tanh\n",
        "        pre_act = (x_t @ model.W_ih.T) + (hs[-1] @ model.W_hh.T) + model.b_h\n",
        "        h_t = torch.tanh(pre_act)\n",
        "        hs.append(h_t)\n",
        "\n",
        "    # final hidden\n",
        "    h_final = hs[-1]\n",
        "    logits = F.linear(h_final, model.fc_out.weight, model.fc_out.bias)\n",
        "\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    loss_val = ce_loss(logits, labels)\n",
        "\n",
        "\n",
        "    # Zero out existing .grad if any\n",
        "    for param in model.parameters():\n",
        "        if param.grad is not None:\n",
        "            param.grad.zero_()\n",
        "    logits.retain_grad()  # so we can read the gradient\n",
        "    # Now do the standard backward\n",
        "    loss_val.backward(retain_graph=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= learning_rate * param.grad\n",
        "\n",
        "    return loss_val.item()\n"
      ],
      "metadata": {
        "id": "208gLG1UaPBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    model = MyManualRNN(vocab_size=20000, embed_dim=100, hidden_size=64, num_classes=2)\n",
        "    model.print_model_summary()\n",
        "\n",
        "    # Example single-batch usage:\n",
        "    input_ids_batch = torch.randint(0, 9999, (8, 50), dtype=torch.long)  # (batch=8, seq_len=50)\n",
        "    labels_batch    = torch.randint(0, 2, (8,), dtype=torch.long)        # binary classes\n",
        "\n",
        "    # Manual training step:\n",
        "    loss_val = manual_backprop(model, input_ids_batch, labels_batch, learning_rate=1e-3)\n",
        "    print(\"Loss:\", loss_val)\n"
      ],
      "metadata": {
        "id": "mt6iXOZoapQD",
        "outputId": "3b9f7980-20c9-44e2-e880-5254453e1aad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_ih: shape=(64, 100), params=6400\n",
            "W_hh: shape=(64, 64), params=4096\n",
            "b_h: shape=(64,), params=64\n",
            "embedding.weight: shape=(20000, 100), params=2000000\n",
            "fc_out.weight: shape=(2, 64), params=128\n",
            "fc_out.bias: shape=(2,), params=2\n",
            "Total trainable parameters: 2010690\n",
            "Loss: 0.8982535004615784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "class IMDBDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Simple PyTorch Dataset wrapper for IMDB data that has already been\n",
        "    tokenized and encoded. Each item is a dict with 'input_ids' and 'label'.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoded_list):\n",
        "        self.encoded_list = encoded_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoded_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.encoded_list[idx]\n",
        "\n",
        "def imdb_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Collate function for DataLoader to create batches:\n",
        "      - Convert lists of 'input_ids' into a tensor\n",
        "      - Convert labels into a tensor\n",
        "    \"\"\"\n",
        "    input_ids_batch = [item[\"input_ids\"] for item in batch]\n",
        "    labels_batch    = [item[\"label\"]     for item in batch]\n",
        "\n",
        "    # Convert to tensors\n",
        "    input_ids_tensor = torch.tensor(input_ids_batch, dtype=torch.long)\n",
        "    labels_tensor    = torch.tensor(labels_batch,   dtype=torch.long)\n",
        "\n",
        "    return input_ids_tensor, labels_tensor\n",
        "\n",
        "def compute_accuracy(logits, labels):\n",
        "    \"\"\"\n",
        "    Compute classification accuracy given model outputs (logits)\n",
        "    and integer class labels.\n",
        "    \"\"\"\n",
        "    preds = torch.argmax(logits, dim=1)\n",
        "    correct = (preds == labels).sum().item()\n",
        "    accuracy = correct / len(labels)\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "NfjVIiDTaty7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_rnn(\n",
        "    model,\n",
        "    train_dataset,\n",
        "    val_dataset=None,\n",
        "    batch_size=32,\n",
        "    epochs=5,\n",
        "    lr=1e-3,\n",
        "    device='cpu'\n",
        "):\n",
        "    \"\"\"\n",
        "    Train the manual RNN on 'train_dataset', optionally validate on 'val_dataset'.\n",
        "    Logs and returns training/validation loss & accuracy per epoch for plotting.\n",
        "    \"\"\"\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=imdb_collate_fn)\n",
        "    val_loader   = None\n",
        "    if val_dataset is not None:\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=imdb_collate_fn)\n",
        "\n",
        "    # Loss & optimizer\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # For logging\n",
        "    train_losses, train_accuracies = [], []\n",
        "    val_losses, val_accuracies     = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        ######################\n",
        "        #  Training Loop\n",
        "        ######################\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        epoch_acc  = 0.0\n",
        "        total_samples = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            input_ids, labels = batch\n",
        "            input_ids = input_ids.to(device)\n",
        "            labels    = labels.to(device)\n",
        "            batch_sz  = input_ids.size(0)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(input_ids)  # shape (batch_size, num_classes)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate stats\n",
        "            epoch_loss += loss.item() * batch_sz\n",
        "            acc = compute_accuracy(logits, labels)\n",
        "            epoch_acc  += acc * batch_sz\n",
        "            total_samples += batch_sz\n",
        "\n",
        "        # Average\n",
        "        epoch_loss /= total_samples\n",
        "        epoch_acc  /= total_samples\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accuracies.append(epoch_acc)\n",
        "\n",
        "        ######################\n",
        "        #  Validation Loop\n",
        "        ######################\n",
        "        val_loss_val, val_acc_val = 0.0, 0.0\n",
        "        val_samples = 0\n",
        "        if val_loader is not None:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    input_ids, labels = batch\n",
        "                    input_ids = input_ids.to(device)\n",
        "                    labels    = labels.to(device)\n",
        "                    batch_sz  = input_ids.size(0)\n",
        "\n",
        "                    logits = model(input_ids)\n",
        "                    loss = criterion(logits, labels)\n",
        "\n",
        "                    val_loss_val += loss.item() * batch_sz\n",
        "                    val_acc_val  += compute_accuracy(logits, labels) * batch_sz\n",
        "                    val_samples  += batch_sz\n",
        "\n",
        "            val_loss_val /= val_samples\n",
        "            val_acc_val  /= val_samples\n",
        "            val_losses.append(val_loss_val)\n",
        "            val_accuracies.append(val_acc_val)\n",
        "\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}] - \"\n",
        "                  f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f} | \"\n",
        "                  f\"Val Loss: {val_loss_val:.4f}, Val Acc: {val_acc_val:.4f}\")\n",
        "        else:\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}] - \"\n",
        "                  f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"train_losses\": train_losses,\n",
        "        \"train_accuracies\": train_accuracies,\n",
        "        \"val_losses\": val_losses,\n",
        "        \"val_accuracies\": val_accuracies\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Olva2x1eqMBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataset, batch_size=32, device='cpu'):\n",
        "    \"\"\"\n",
        "    Evaluates the model on a given dataset, computing accuracy,\n",
        "    plus optional P/R/F for deeper insight.\n",
        "    \"\"\"\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=imdb_collate_fn)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids, labels = batch\n",
        "            input_ids = input_ids.to(device)\n",
        "            labels    = labels.to(device)\n",
        "            batch_sz  = input_ids.size(0)\n",
        "\n",
        "            logits = model(input_ids)\n",
        "            loss   = criterion(logits, labels)\n",
        "\n",
        "            total_loss += loss.item() * batch_sz\n",
        "            total_samples += batch_sz\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_labels.append(labels.cpu())\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    all_preds  = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    accuracy = (all_preds == all_labels).sum().item() / total_samples\n",
        "\n",
        "    # Optional: compute precision, recall, F1\n",
        "    # We'll use sklearn for that\n",
        "    precision = precision_score(all_labels, all_preds, average='binary')\n",
        "    recall    = recall_score(all_labels, all_preds, average='binary')\n",
        "    f1        = f1_score(all_labels, all_preds, average='binary')\n",
        "\n",
        "    return {\n",
        "        \"loss\": avg_loss,\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n"
      ],
      "metadata": {
        "id": "9GqJ6diKqRl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss_curves(train_losses, val_losses=None):\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.plot(train_losses, label=\"Train Loss\")\n",
        "    if val_losses:\n",
        "        plt.plot(val_losses, label=\"Val Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training (and Validation) Loss Over Epochs\")\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "vWzWG6gJqUI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_predictions(model, dataset, device='cpu', num_examples=5):\n",
        "    \"\"\"\n",
        "    Print out a few examples of model predictions vs. ground truth.\n",
        "    \"\"\"\n",
        "    loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=imdb_collate_fn)\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    count = 0\n",
        "    for batch in loader:\n",
        "        if count >= num_examples:\n",
        "            break\n",
        "        input_ids, labels = batch\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels    = labels.item()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_ids)\n",
        "            pred = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "        # Print details\n",
        "        print(f\"Example {count+1}: True Label = {labels}, Predicted = {pred}\")\n",
        "        count += 1\n"
      ],
      "metadata": {
        "id": "7ToOmVFKqXPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # 1) Suppose you have train_encoded, val_encoded, test_encoded from part (a):\n",
        "    train_dataset = IMDBDataset(train_encoded)\n",
        "    val_dataset   = IMDBDataset(val_encoded)\n",
        "    test_dataset  = IMDBDataset(test_encoded)\n",
        "\n",
        "    # 2) Suppose you have a MyManualRNN or similar from part (b),\n",
        "\n",
        "    my_rnn = MyManualRNN(vocab_size=20000, embed_dim=100, hidden_size=64, num_classes=2)\n",
        "\n",
        "    # 3) Train the RNN\n",
        "    history = train_rnn(\n",
        "        model=my_rnn,\n",
        "        train_dataset=train_dataset,\n",
        "        val_dataset=val_dataset,\n",
        "        batch_size=32,\n",
        "        epochs=5,\n",
        "        lr=0.01,\n",
        "        device='cuda'  # or 'cpu'\n",
        "    )\n",
        "\n",
        "    # 4) Plot training/val loss\n",
        "    plot_loss_curves(history[\"train_losses\"], history[\"val_losses\"])\n",
        "\n",
        "    # 5) Evaluate on test set\n",
        "    test_metrics = evaluate_model(my_rnn, test_dataset, batch_size=32, device='cuda')\n",
        "    print(\"Test Metrics:\", test_metrics)\n",
        "\n",
        "    # 6) Visualize a few predictions\n",
        "    visualize_predictions(my_rnn, test_dataset, device='cuda', num_examples=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "id": "J1P3c1D7qYRl",
        "outputId": "ee9050db-d27d-4372-dfe8-b3c017870f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5] - Train Loss: 0.7154, Train Acc: 0.5023 | Val Loss: 0.7027, Val Acc: 0.5076\n",
            "Epoch [2/5] - Train Loss: 0.6790, Train Acc: 0.5554 | Val Loss: 0.7028, Val Acc: 0.5036\n",
            "Epoch [3/5] - Train Loss: 0.6521, Train Acc: 0.5851 | Val Loss: 0.7130, Val Acc: 0.5164\n",
            "Epoch [4/5] - Train Loss: 0.6071, Train Acc: 0.6176 | Val Loss: 0.7375, Val Acc: 0.5118\n",
            "Epoch [5/5] - Train Loss: 0.5508, Train Acc: 0.6513 | Val Loss: 0.7859, Val Acc: 0.5102\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Metrics: {'loss': 0.7818151849555969, 'accuracy': 0.51096, 'precision': 0.5068506850685068, 'recall': 0.81088, 'f1': 0.6237922333682073}\n",
            "Example 1: True Label = 1, Predicted = 1\n",
            "Example 2: True Label = 0, Predicted = 1\n",
            "Example 3: True Label = 0, Predicted = 0\n",
            "Example 4: True Label = 1, Predicted = 0\n",
            "Example 5: True Label = 0, Predicted = 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Performance Analysis\n",
        "Metrics and Visualizations:\n",
        "The training accuracy steadily rises (reaching ~65%), while validation accuracy hovers around ~50%. The training loss decreases significantly (down to ~0.55), but validation loss increases from ~0.70 to ~0.79, indicating overfitting—the model memorizes training data but doesn’t generalize well on unseen samples.\n",
        "\n",
        "Generalization to Unseen Data:\n",
        "The test accuracy (~51%) suggests the model struggles with new reviews. Despite improving on the training set, it barely outperforms random guessing for validation/test data. This gap implies that additional regularization or more sophisticated modeling is needed.\n",
        "\n",
        "Strengths & Weaknesses of Manual Implementation:\n",
        "\n",
        "Strengths:\n",
        "Full transparency into how hidden states are updated each time step.\n",
        "Ability to easily experiment with custom modifications at a very fine-grained level.\n",
        "Weaknesses:\n",
        "More verbose code and a higher chance of bugs.\n",
        "Slower to train than PyTorch’s optimized RNN modules (no cuDNN acceleration).\n",
        "Susceptible to vanishing/exploding gradients for longer sequences.\n"
      ],
      "metadata": {
        "id": "WqtqCHzAT10t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Comparison to Built-in Modules\n",
        "Computational Efficiency:\n",
        "PyTorch’s built-in nn.RNN typically leverages cuDNN and other optimizations, running faster and handling large batches more efficiently. The manual loop in Python is comparatively slow, especially for longer sequences.\n",
        "\n",
        "Implementation Complexity:\n",
        "The built-in nn.RNN  approach is concise—just a few lines to define and run the model—whereas the manual approach requires carefully coding weight matrices, hidden-state updates, and bias terms.\n",
        "\n",
        "Flexibility:\n",
        "A manual RNN can be adapted in ways that standard modules cannot—such as adding custom gating or attention mechanisms at the code level. For most production tasks, though, the built-in modules are sufficiently flexible, less error-prone, and more performant."
      ],
      "metadata": {
        "id": "71xTM9wtUFG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Challenges and Improvements\n",
        "Challenges:\n",
        "\n",
        "Overfitting: The validation loss climbs while training loss drops, showing the model fails to generalize.\n",
        "Training Stability: Vanilla RNNs commonly face issues like vanishing gradients when dealing with long reviews.\n",
        "Time-Consuming: Implementing manual backprop through time is prone to errors and is computationally slower.\n",
        "Potential Improvements:\n",
        "\n",
        "Regularization: we have to oncorporate dropout on embeddings or hidden states, use weight decay in the optimizer, or employ early stopping based on validation metrics.\n",
        "\n",
        "Architecture Enhancements: We need to replace the simple RNN with an LSTM or GRU cell to better capture long-term dependencies and mitigate vanishing gradients.\n",
        "\n",
        "Better Preprocessing: We need to use more sophisticated tokenization or pretrained embeddings to give the model richer linguistic knowledge.\n",
        "\n",
        "Hyperparameter Tuning: To experiment with different hidden sizes, batch sizes, and learning rates."
      ],
      "metadata": {
        "id": "HMyWNrGbUQN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. N-gram language model to analyze"
      ],
      "metadata": {
        "id": "TNS-luuPSwcW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Exploration:"
      ],
      "metadata": {
        "id": "__0setocS23G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrJka1FaTw5g",
        "outputId": "43748c6e-9c2c-4bf1-fa4f-0264513f9b4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define file paths (update with actual paths in Google Drive)\n",
        "lyrics_path = \"/content/drive/MyDrive/Colab Notebooks/lyrics-data.csv\"\n",
        "artists_path = \"/content/drive/MyDrive/Colab Notebooks/artists-data.csv\"\n",
        "\n",
        "from re import sub\n",
        "\n",
        "df_artists = pd.read_csv(artists_path)\n",
        "df_lyrics = pd.read_csv(lyrics_path)\n",
        "\n",
        "# Rename 'ALink' to 'Link' for easier merging\n",
        "df_lyrics.rename(columns={'ALink': 'Link'}, inplace=True)\n",
        "\n",
        "# Merge datasets where language is English\n",
        "song_df = pd.merge(df_lyrics[df_lyrics['language'] == 'en'], df_artists, on='Link', how=\"left\")\n",
        "\n",
        "# Drop missing values in lyrics\n",
        "song_df.dropna(subset=['Lyric'], inplace=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DM8kGwRIUsSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why is data cleaning necessary?\n",
        "\n",
        "* Before training an N-Gram model for lyric generation, cleaning the data ensures:\n",
        "\n",
        "* Data consistency → Prevents issues caused by missing or malformed entries.\n",
        "* Better tokenization → Removing unwanted characters improves model performance.\n",
        "* Accurate genre classification → Ensures only relevant songs are included.\n",
        "* Improved generalization → Avoids overfitting to noise or incorrect data."
      ],
      "metadata": {
        "id": "79f-eL6lI0mD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Genres of interest\n",
        "genres_of_interest = ['Hip Hop', 'Rock Alternativo', 'Disco']\n",
        "\n",
        "# Initialize an empty dictionary to store lyrics for each genre\n",
        "genre_lyrics = {genre: [] for genre in genres_of_interest}\n",
        "\n",
        "# Iterate through the DataFrame and extract lyrics for the specified genres\n",
        "for index, row in song_df.iterrows():\n",
        "    if isinstance(row['Genres'], str):\n",
        "        genres = row['Genres'].split(';')\n",
        "        lyrics = row['Lyric']\n",
        "        for genre in genres:\n",
        "            if genre in genres_of_interest:\n",
        "                genre_lyrics[genre].append(lyrics)\n",
        "    else:\n",
        "        print(f\"Skipping row {index} due to invalid 'Genres' value: {row['Genres']}\")\n",
        "\n",
        "# Print the number of lyrics found for each genre\n",
        "for genre, lyrics_list in genre_lyrics.items():\n",
        "    print(f\"Number of lyrics for {genre}: {len(lyrics_list)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGI8UZLdXiJl",
        "outputId": "67b0c361-6e73-465c-c48b-651bd1d3ed02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping row 16808 due to invalid 'Genres' value: nan\n",
            "Skipping row 16809 due to invalid 'Genres' value: nan\n",
            "Skipping row 17440 due to invalid 'Genres' value: nan\n",
            "Skipping row 17441 due to invalid 'Genres' value: nan\n",
            "Skipping row 17442 due to invalid 'Genres' value: nan\n",
            "Skipping row 17443 due to invalid 'Genres' value: nan\n",
            "Skipping row 17444 due to invalid 'Genres' value: nan\n",
            "Skipping row 17445 due to invalid 'Genres' value: nan\n",
            "Skipping row 17446 due to invalid 'Genres' value: nan\n",
            "Skipping row 17447 due to invalid 'Genres' value: nan\n",
            "Skipping row 17448 due to invalid 'Genres' value: nan\n",
            "Skipping row 17449 due to invalid 'Genres' value: nan\n",
            "Skipping row 17450 due to invalid 'Genres' value: nan\n",
            "Skipping row 17926 due to invalid 'Genres' value: nan\n",
            "Skipping row 17927 due to invalid 'Genres' value: nan\n",
            "Skipping row 17928 due to invalid 'Genres' value: nan\n",
            "Skipping row 17929 due to invalid 'Genres' value: nan\n",
            "Skipping row 28557 due to invalid 'Genres' value: nan\n",
            "Skipping row 45160 due to invalid 'Genres' value: nan\n",
            "Skipping row 45264 due to invalid 'Genres' value: nan\n",
            "Skipping row 57591 due to invalid 'Genres' value: nan\n",
            "Skipping row 57592 due to invalid 'Genres' value: nan\n",
            "Skipping row 57593 due to invalid 'Genres' value: nan\n",
            "Skipping row 57594 due to invalid 'Genres' value: nan\n",
            "Skipping row 57595 due to invalid 'Genres' value: nan\n",
            "Skipping row 57596 due to invalid 'Genres' value: nan\n",
            "Skipping row 57597 due to invalid 'Genres' value: nan\n",
            "Skipping row 57598 due to invalid 'Genres' value: nan\n",
            "Skipping row 57599 due to invalid 'Genres' value: nan\n",
            "Skipping row 57600 due to invalid 'Genres' value: nan\n",
            "Skipping row 57601 due to invalid 'Genres' value: nan\n",
            "Skipping row 57602 due to invalid 'Genres' value: nan\n",
            "Skipping row 57603 due to invalid 'Genres' value: nan\n",
            "Skipping row 57604 due to invalid 'Genres' value: nan\n",
            "Skipping row 57605 due to invalid 'Genres' value: nan\n",
            "Skipping row 57606 due to invalid 'Genres' value: nan\n",
            "Skipping row 57607 due to invalid 'Genres' value: nan\n",
            "Skipping row 57608 due to invalid 'Genres' value: nan\n",
            "Skipping row 57609 due to invalid 'Genres' value: nan\n",
            "Skipping row 57610 due to invalid 'Genres' value: nan\n",
            "Skipping row 57611 due to invalid 'Genres' value: nan\n",
            "Skipping row 57612 due to invalid 'Genres' value: nan\n",
            "Skipping row 57613 due to invalid 'Genres' value: nan\n",
            "Skipping row 57614 due to invalid 'Genres' value: nan\n",
            "Skipping row 57615 due to invalid 'Genres' value: nan\n",
            "Skipping row 57616 due to invalid 'Genres' value: nan\n",
            "Skipping row 57617 due to invalid 'Genres' value: nan\n",
            "Skipping row 57618 due to invalid 'Genres' value: nan\n",
            "Skipping row 57619 due to invalid 'Genres' value: nan\n",
            "Skipping row 57620 due to invalid 'Genres' value: nan\n",
            "Skipping row 57621 due to invalid 'Genres' value: nan\n",
            "Skipping row 57622 due to invalid 'Genres' value: nan\n",
            "Skipping row 57623 due to invalid 'Genres' value: nan\n",
            "Skipping row 57624 due to invalid 'Genres' value: nan\n",
            "Skipping row 57625 due to invalid 'Genres' value: nan\n",
            "Skipping row 57626 due to invalid 'Genres' value: nan\n",
            "Skipping row 57627 due to invalid 'Genres' value: nan\n",
            "Skipping row 57628 due to invalid 'Genres' value: nan\n",
            "Skipping row 57629 due to invalid 'Genres' value: nan\n",
            "Skipping row 57630 due to invalid 'Genres' value: nan\n",
            "Skipping row 57631 due to invalid 'Genres' value: nan\n",
            "Skipping row 57632 due to invalid 'Genres' value: nan\n",
            "Skipping row 57633 due to invalid 'Genres' value: nan\n",
            "Skipping row 57634 due to invalid 'Genres' value: nan\n",
            "Skipping row 57635 due to invalid 'Genres' value: nan\n",
            "Skipping row 57636 due to invalid 'Genres' value: nan\n",
            "Skipping row 57637 due to invalid 'Genres' value: nan\n",
            "Skipping row 57638 due to invalid 'Genres' value: nan\n",
            "Skipping row 57639 due to invalid 'Genres' value: nan\n",
            "Skipping row 57640 due to invalid 'Genres' value: nan\n",
            "Skipping row 57641 due to invalid 'Genres' value: nan\n",
            "Skipping row 57642 due to invalid 'Genres' value: nan\n",
            "Skipping row 57643 due to invalid 'Genres' value: nan\n",
            "Skipping row 57644 due to invalid 'Genres' value: nan\n",
            "Skipping row 57645 due to invalid 'Genres' value: nan\n",
            "Skipping row 57646 due to invalid 'Genres' value: nan\n",
            "Skipping row 57647 due to invalid 'Genres' value: nan\n",
            "Skipping row 57648 due to invalid 'Genres' value: nan\n",
            "Skipping row 57649 due to invalid 'Genres' value: nan\n",
            "Skipping row 57650 due to invalid 'Genres' value: nan\n",
            "Skipping row 57651 due to invalid 'Genres' value: nan\n",
            "Skipping row 57652 due to invalid 'Genres' value: nan\n",
            "Skipping row 57653 due to invalid 'Genres' value: nan\n",
            "Skipping row 57654 due to invalid 'Genres' value: nan\n",
            "Skipping row 57655 due to invalid 'Genres' value: nan\n",
            "Skipping row 57656 due to invalid 'Genres' value: nan\n",
            "Skipping row 57657 due to invalid 'Genres' value: nan\n",
            "Skipping row 57658 due to invalid 'Genres' value: nan\n",
            "Skipping row 57659 due to invalid 'Genres' value: nan\n",
            "Skipping row 57660 due to invalid 'Genres' value: nan\n",
            "Skipping row 57661 due to invalid 'Genres' value: nan\n",
            "Skipping row 57662 due to invalid 'Genres' value: nan\n",
            "Skipping row 57663 due to invalid 'Genres' value: nan\n",
            "Skipping row 57664 due to invalid 'Genres' value: nan\n",
            "Skipping row 57665 due to invalid 'Genres' value: nan\n",
            "Skipping row 57666 due to invalid 'Genres' value: nan\n",
            "Skipping row 57667 due to invalid 'Genres' value: nan\n",
            "Skipping row 57668 due to invalid 'Genres' value: nan\n",
            "Skipping row 57669 due to invalid 'Genres' value: nan\n",
            "Skipping row 57670 due to invalid 'Genres' value: nan\n",
            "Skipping row 57671 due to invalid 'Genres' value: nan\n",
            "Skipping row 57672 due to invalid 'Genres' value: nan\n",
            "Skipping row 57673 due to invalid 'Genres' value: nan\n",
            "Skipping row 57674 due to invalid 'Genres' value: nan\n",
            "Skipping row 57675 due to invalid 'Genres' value: nan\n",
            "Skipping row 57676 due to invalid 'Genres' value: nan\n",
            "Skipping row 57677 due to invalid 'Genres' value: nan\n",
            "Skipping row 57678 due to invalid 'Genres' value: nan\n",
            "Skipping row 57679 due to invalid 'Genres' value: nan\n",
            "Skipping row 57680 due to invalid 'Genres' value: nan\n",
            "Skipping row 57681 due to invalid 'Genres' value: nan\n",
            "Skipping row 57682 due to invalid 'Genres' value: nan\n",
            "Skipping row 57683 due to invalid 'Genres' value: nan\n",
            "Skipping row 57684 due to invalid 'Genres' value: nan\n",
            "Skipping row 57685 due to invalid 'Genres' value: nan\n",
            "Skipping row 57686 due to invalid 'Genres' value: nan\n",
            "Skipping row 57687 due to invalid 'Genres' value: nan\n",
            "Skipping row 57688 due to invalid 'Genres' value: nan\n",
            "Skipping row 57689 due to invalid 'Genres' value: nan\n",
            "Skipping row 57690 due to invalid 'Genres' value: nan\n",
            "Skipping row 57691 due to invalid 'Genres' value: nan\n",
            "Skipping row 57692 due to invalid 'Genres' value: nan\n",
            "Skipping row 57693 due to invalid 'Genres' value: nan\n",
            "Skipping row 57694 due to invalid 'Genres' value: nan\n",
            "Skipping row 57695 due to invalid 'Genres' value: nan\n",
            "Skipping row 57696 due to invalid 'Genres' value: nan\n",
            "Skipping row 57697 due to invalid 'Genres' value: nan\n",
            "Skipping row 57698 due to invalid 'Genres' value: nan\n",
            "Skipping row 57699 due to invalid 'Genres' value: nan\n",
            "Skipping row 57700 due to invalid 'Genres' value: nan\n",
            "Skipping row 57701 due to invalid 'Genres' value: nan\n",
            "Skipping row 57702 due to invalid 'Genres' value: nan\n",
            "Skipping row 57703 due to invalid 'Genres' value: nan\n",
            "Skipping row 57704 due to invalid 'Genres' value: nan\n",
            "Skipping row 57705 due to invalid 'Genres' value: nan\n",
            "Skipping row 57706 due to invalid 'Genres' value: nan\n",
            "Skipping row 57707 due to invalid 'Genres' value: nan\n",
            "Skipping row 57708 due to invalid 'Genres' value: nan\n",
            "Skipping row 57709 due to invalid 'Genres' value: nan\n",
            "Skipping row 57710 due to invalid 'Genres' value: nan\n",
            "Skipping row 57711 due to invalid 'Genres' value: nan\n",
            "Skipping row 57712 due to invalid 'Genres' value: nan\n",
            "Skipping row 57713 due to invalid 'Genres' value: nan\n",
            "Skipping row 57714 due to invalid 'Genres' value: nan\n",
            "Skipping row 57715 due to invalid 'Genres' value: nan\n",
            "Skipping row 57716 due to invalid 'Genres' value: nan\n",
            "Skipping row 57717 due to invalid 'Genres' value: nan\n",
            "Skipping row 57718 due to invalid 'Genres' value: nan\n",
            "Skipping row 57719 due to invalid 'Genres' value: nan\n",
            "Skipping row 57720 due to invalid 'Genres' value: nan\n",
            "Skipping row 57721 due to invalid 'Genres' value: nan\n",
            "Skipping row 57722 due to invalid 'Genres' value: nan\n",
            "Skipping row 57723 due to invalid 'Genres' value: nan\n",
            "Skipping row 57724 due to invalid 'Genres' value: nan\n",
            "Skipping row 57725 due to invalid 'Genres' value: nan\n",
            "Skipping row 57726 due to invalid 'Genres' value: nan\n",
            "Skipping row 57727 due to invalid 'Genres' value: nan\n",
            "Skipping row 57728 due to invalid 'Genres' value: nan\n",
            "Skipping row 57729 due to invalid 'Genres' value: nan\n",
            "Skipping row 57730 due to invalid 'Genres' value: nan\n",
            "Skipping row 57731 due to invalid 'Genres' value: nan\n",
            "Skipping row 57732 due to invalid 'Genres' value: nan\n",
            "Skipping row 57733 due to invalid 'Genres' value: nan\n",
            "Skipping row 57734 due to invalid 'Genres' value: nan\n",
            "Skipping row 57735 due to invalid 'Genres' value: nan\n",
            "Skipping row 57736 due to invalid 'Genres' value: nan\n",
            "Skipping row 57737 due to invalid 'Genres' value: nan\n",
            "Skipping row 57738 due to invalid 'Genres' value: nan\n",
            "Skipping row 57739 due to invalid 'Genres' value: nan\n",
            "Skipping row 57740 due to invalid 'Genres' value: nan\n",
            "Skipping row 57741 due to invalid 'Genres' value: nan\n",
            "Skipping row 57742 due to invalid 'Genres' value: nan\n",
            "Skipping row 57743 due to invalid 'Genres' value: nan\n",
            "Skipping row 57744 due to invalid 'Genres' value: nan\n",
            "Skipping row 57745 due to invalid 'Genres' value: nan\n",
            "Skipping row 57746 due to invalid 'Genres' value: nan\n",
            "Skipping row 57747 due to invalid 'Genres' value: nan\n",
            "Skipping row 57748 due to invalid 'Genres' value: nan\n",
            "Skipping row 57749 due to invalid 'Genres' value: nan\n",
            "Skipping row 57750 due to invalid 'Genres' value: nan\n",
            "Skipping row 57751 due to invalid 'Genres' value: nan\n",
            "Skipping row 57752 due to invalid 'Genres' value: nan\n",
            "Skipping row 57753 due to invalid 'Genres' value: nan\n",
            "Skipping row 57754 due to invalid 'Genres' value: nan\n",
            "Skipping row 57755 due to invalid 'Genres' value: nan\n",
            "Skipping row 57756 due to invalid 'Genres' value: nan\n",
            "Skipping row 57757 due to invalid 'Genres' value: nan\n",
            "Skipping row 57758 due to invalid 'Genres' value: nan\n",
            "Skipping row 57759 due to invalid 'Genres' value: nan\n",
            "Skipping row 57760 due to invalid 'Genres' value: nan\n",
            "Skipping row 57761 due to invalid 'Genres' value: nan\n",
            "Skipping row 57762 due to invalid 'Genres' value: nan\n",
            "Skipping row 57763 due to invalid 'Genres' value: nan\n",
            "Skipping row 57764 due to invalid 'Genres' value: nan\n",
            "Skipping row 57765 due to invalid 'Genres' value: nan\n",
            "Skipping row 57766 due to invalid 'Genres' value: nan\n",
            "Skipping row 57767 due to invalid 'Genres' value: nan\n",
            "Skipping row 57768 due to invalid 'Genres' value: nan\n",
            "Skipping row 57769 due to invalid 'Genres' value: nan\n",
            "Skipping row 57770 due to invalid 'Genres' value: nan\n",
            "Skipping row 57771 due to invalid 'Genres' value: nan\n",
            "Skipping row 57772 due to invalid 'Genres' value: nan\n",
            "Skipping row 57773 due to invalid 'Genres' value: nan\n",
            "Skipping row 57774 due to invalid 'Genres' value: nan\n",
            "Skipping row 57775 due to invalid 'Genres' value: nan\n",
            "Skipping row 57776 due to invalid 'Genres' value: nan\n",
            "Skipping row 57777 due to invalid 'Genres' value: nan\n",
            "Skipping row 57778 due to invalid 'Genres' value: nan\n",
            "Skipping row 57779 due to invalid 'Genres' value: nan\n",
            "Skipping row 57780 due to invalid 'Genres' value: nan\n",
            "Skipping row 57781 due to invalid 'Genres' value: nan\n",
            "Skipping row 57782 due to invalid 'Genres' value: nan\n",
            "Skipping row 57783 due to invalid 'Genres' value: nan\n",
            "Skipping row 57784 due to invalid 'Genres' value: nan\n",
            "Skipping row 57785 due to invalid 'Genres' value: nan\n",
            "Skipping row 57786 due to invalid 'Genres' value: nan\n",
            "Skipping row 57787 due to invalid 'Genres' value: nan\n",
            "Skipping row 57788 due to invalid 'Genres' value: nan\n",
            "Skipping row 57789 due to invalid 'Genres' value: nan\n",
            "Skipping row 57790 due to invalid 'Genres' value: nan\n",
            "Skipping row 57791 due to invalid 'Genres' value: nan\n",
            "Skipping row 57792 due to invalid 'Genres' value: nan\n",
            "Skipping row 57793 due to invalid 'Genres' value: nan\n",
            "Skipping row 57794 due to invalid 'Genres' value: nan\n",
            "Skipping row 57795 due to invalid 'Genres' value: nan\n",
            "Skipping row 57796 due to invalid 'Genres' value: nan\n",
            "Skipping row 57797 due to invalid 'Genres' value: nan\n",
            "Skipping row 57798 due to invalid 'Genres' value: nan\n",
            "Skipping row 57799 due to invalid 'Genres' value: nan\n",
            "Skipping row 57800 due to invalid 'Genres' value: nan\n",
            "Skipping row 57801 due to invalid 'Genres' value: nan\n",
            "Skipping row 57802 due to invalid 'Genres' value: nan\n",
            "Skipping row 57803 due to invalid 'Genres' value: nan\n",
            "Skipping row 57804 due to invalid 'Genres' value: nan\n",
            "Skipping row 57805 due to invalid 'Genres' value: nan\n",
            "Skipping row 57806 due to invalid 'Genres' value: nan\n",
            "Skipping row 57807 due to invalid 'Genres' value: nan\n",
            "Skipping row 57808 due to invalid 'Genres' value: nan\n",
            "Skipping row 57809 due to invalid 'Genres' value: nan\n",
            "Skipping row 57810 due to invalid 'Genres' value: nan\n",
            "Skipping row 57811 due to invalid 'Genres' value: nan\n",
            "Skipping row 57812 due to invalid 'Genres' value: nan\n",
            "Skipping row 57813 due to invalid 'Genres' value: nan\n",
            "Skipping row 57814 due to invalid 'Genres' value: nan\n",
            "Skipping row 57815 due to invalid 'Genres' value: nan\n",
            "Skipping row 57816 due to invalid 'Genres' value: nan\n",
            "Skipping row 57817 due to invalid 'Genres' value: nan\n",
            "Skipping row 57818 due to invalid 'Genres' value: nan\n",
            "Skipping row 57819 due to invalid 'Genres' value: nan\n",
            "Skipping row 57820 due to invalid 'Genres' value: nan\n",
            "Skipping row 57821 due to invalid 'Genres' value: nan\n",
            "Skipping row 57822 due to invalid 'Genres' value: nan\n",
            "Skipping row 57823 due to invalid 'Genres' value: nan\n",
            "Skipping row 57824 due to invalid 'Genres' value: nan\n",
            "Skipping row 57825 due to invalid 'Genres' value: nan\n",
            "Skipping row 57826 due to invalid 'Genres' value: nan\n",
            "Skipping row 57827 due to invalid 'Genres' value: nan\n",
            "Skipping row 57828 due to invalid 'Genres' value: nan\n",
            "Skipping row 57829 due to invalid 'Genres' value: nan\n",
            "Skipping row 57830 due to invalid 'Genres' value: nan\n",
            "Skipping row 57831 due to invalid 'Genres' value: nan\n",
            "Skipping row 57832 due to invalid 'Genres' value: nan\n",
            "Skipping row 57833 due to invalid 'Genres' value: nan\n",
            "Skipping row 57834 due to invalid 'Genres' value: nan\n",
            "Skipping row 57835 due to invalid 'Genres' value: nan\n",
            "Skipping row 57836 due to invalid 'Genres' value: nan\n",
            "Skipping row 57837 due to invalid 'Genres' value: nan\n",
            "Skipping row 57838 due to invalid 'Genres' value: nan\n",
            "Skipping row 57839 due to invalid 'Genres' value: nan\n",
            "Skipping row 57840 due to invalid 'Genres' value: nan\n",
            "Skipping row 57841 due to invalid 'Genres' value: nan\n",
            "Skipping row 57842 due to invalid 'Genres' value: nan\n",
            "Skipping row 57843 due to invalid 'Genres' value: nan\n",
            "Skipping row 57844 due to invalid 'Genres' value: nan\n",
            "Skipping row 57845 due to invalid 'Genres' value: nan\n",
            "Skipping row 57846 due to invalid 'Genres' value: nan\n",
            "Skipping row 57847 due to invalid 'Genres' value: nan\n",
            "Skipping row 57848 due to invalid 'Genres' value: nan\n",
            "Skipping row 57849 due to invalid 'Genres' value: nan\n",
            "Skipping row 57850 due to invalid 'Genres' value: nan\n",
            "Skipping row 57851 due to invalid 'Genres' value: nan\n",
            "Skipping row 57852 due to invalid 'Genres' value: nan\n",
            "Skipping row 57853 due to invalid 'Genres' value: nan\n",
            "Skipping row 57854 due to invalid 'Genres' value: nan\n",
            "Skipping row 57855 due to invalid 'Genres' value: nan\n",
            "Skipping row 57856 due to invalid 'Genres' value: nan\n",
            "Skipping row 57857 due to invalid 'Genres' value: nan\n",
            "Skipping row 57858 due to invalid 'Genres' value: nan\n",
            "Skipping row 57859 due to invalid 'Genres' value: nan\n",
            "Skipping row 57860 due to invalid 'Genres' value: nan\n",
            "Skipping row 57861 due to invalid 'Genres' value: nan\n",
            "Skipping row 57862 due to invalid 'Genres' value: nan\n",
            "Skipping row 57863 due to invalid 'Genres' value: nan\n",
            "Skipping row 57864 due to invalid 'Genres' value: nan\n",
            "Skipping row 57865 due to invalid 'Genres' value: nan\n",
            "Skipping row 57866 due to invalid 'Genres' value: nan\n",
            "Skipping row 57867 due to invalid 'Genres' value: nan\n",
            "Skipping row 57868 due to invalid 'Genres' value: nan\n",
            "Skipping row 57869 due to invalid 'Genres' value: nan\n",
            "Skipping row 57870 due to invalid 'Genres' value: nan\n",
            "Skipping row 57871 due to invalid 'Genres' value: nan\n",
            "Skipping row 57872 due to invalid 'Genres' value: nan\n",
            "Skipping row 57873 due to invalid 'Genres' value: nan\n",
            "Skipping row 57874 due to invalid 'Genres' value: nan\n",
            "Skipping row 57875 due to invalid 'Genres' value: nan\n",
            "Skipping row 57876 due to invalid 'Genres' value: nan\n",
            "Skipping row 57877 due to invalid 'Genres' value: nan\n",
            "Skipping row 57878 due to invalid 'Genres' value: nan\n",
            "Skipping row 57879 due to invalid 'Genres' value: nan\n",
            "Skipping row 57880 due to invalid 'Genres' value: nan\n",
            "Skipping row 57881 due to invalid 'Genres' value: nan\n",
            "Skipping row 57882 due to invalid 'Genres' value: nan\n",
            "Skipping row 57883 due to invalid 'Genres' value: nan\n",
            "Skipping row 57884 due to invalid 'Genres' value: nan\n",
            "Skipping row 57885 due to invalid 'Genres' value: nan\n",
            "Skipping row 57886 due to invalid 'Genres' value: nan\n",
            "Skipping row 57887 due to invalid 'Genres' value: nan\n",
            "Skipping row 57888 due to invalid 'Genres' value: nan\n",
            "Skipping row 57889 due to invalid 'Genres' value: nan\n",
            "Skipping row 57890 due to invalid 'Genres' value: nan\n",
            "Skipping row 57891 due to invalid 'Genres' value: nan\n",
            "Skipping row 57892 due to invalid 'Genres' value: nan\n",
            "Skipping row 57893 due to invalid 'Genres' value: nan\n",
            "Skipping row 57894 due to invalid 'Genres' value: nan\n",
            "Skipping row 57895 due to invalid 'Genres' value: nan\n",
            "Skipping row 57896 due to invalid 'Genres' value: nan\n",
            "Skipping row 57897 due to invalid 'Genres' value: nan\n",
            "Skipping row 57898 due to invalid 'Genres' value: nan\n",
            "Skipping row 57899 due to invalid 'Genres' value: nan\n",
            "Skipping row 57900 due to invalid 'Genres' value: nan\n",
            "Skipping row 57901 due to invalid 'Genres' value: nan\n",
            "Skipping row 57902 due to invalid 'Genres' value: nan\n",
            "Skipping row 57903 due to invalid 'Genres' value: nan\n",
            "Skipping row 57904 due to invalid 'Genres' value: nan\n",
            "Skipping row 57905 due to invalid 'Genres' value: nan\n",
            "Skipping row 57906 due to invalid 'Genres' value: nan\n",
            "Skipping row 57907 due to invalid 'Genres' value: nan\n",
            "Skipping row 57908 due to invalid 'Genres' value: nan\n",
            "Skipping row 57909 due to invalid 'Genres' value: nan\n",
            "Skipping row 57910 due to invalid 'Genres' value: nan\n",
            "Skipping row 57911 due to invalid 'Genres' value: nan\n",
            "Skipping row 57912 due to invalid 'Genres' value: nan\n",
            "Skipping row 57913 due to invalid 'Genres' value: nan\n",
            "Skipping row 57914 due to invalid 'Genres' value: nan\n",
            "Skipping row 57915 due to invalid 'Genres' value: nan\n",
            "Skipping row 57916 due to invalid 'Genres' value: nan\n",
            "Skipping row 57917 due to invalid 'Genres' value: nan\n",
            "Skipping row 57918 due to invalid 'Genres' value: nan\n",
            "Skipping row 57919 due to invalid 'Genres' value: nan\n",
            "Skipping row 57920 due to invalid 'Genres' value: nan\n",
            "Skipping row 57921 due to invalid 'Genres' value: nan\n",
            "Skipping row 57922 due to invalid 'Genres' value: nan\n",
            "Skipping row 57923 due to invalid 'Genres' value: nan\n",
            "Skipping row 57924 due to invalid 'Genres' value: nan\n",
            "Skipping row 57925 due to invalid 'Genres' value: nan\n",
            "Skipping row 57926 due to invalid 'Genres' value: nan\n",
            "Skipping row 57927 due to invalid 'Genres' value: nan\n",
            "Skipping row 57928 due to invalid 'Genres' value: nan\n",
            "Skipping row 57929 due to invalid 'Genres' value: nan\n",
            "Skipping row 57930 due to invalid 'Genres' value: nan\n",
            "Skipping row 57931 due to invalid 'Genres' value: nan\n",
            "Skipping row 57932 due to invalid 'Genres' value: nan\n",
            "Skipping row 57933 due to invalid 'Genres' value: nan\n",
            "Skipping row 57934 due to invalid 'Genres' value: nan\n",
            "Skipping row 57935 due to invalid 'Genres' value: nan\n",
            "Skipping row 57936 due to invalid 'Genres' value: nan\n",
            "Skipping row 57937 due to invalid 'Genres' value: nan\n",
            "Skipping row 57938 due to invalid 'Genres' value: nan\n",
            "Skipping row 57939 due to invalid 'Genres' value: nan\n",
            "Skipping row 57940 due to invalid 'Genres' value: nan\n",
            "Skipping row 61234 due to invalid 'Genres' value: nan\n",
            "Skipping row 91204 due to invalid 'Genres' value: nan\n",
            "Skipping row 91205 due to invalid 'Genres' value: nan\n",
            "Skipping row 91206 due to invalid 'Genres' value: nan\n",
            "Skipping row 91207 due to invalid 'Genres' value: nan\n",
            "Skipping row 91208 due to invalid 'Genres' value: nan\n",
            "Skipping row 91209 due to invalid 'Genres' value: nan\n",
            "Skipping row 91210 due to invalid 'Genres' value: nan\n",
            "Skipping row 91211 due to invalid 'Genres' value: nan\n",
            "Skipping row 91212 due to invalid 'Genres' value: nan\n",
            "Skipping row 103415 due to invalid 'Genres' value: nan\n",
            "Skipping row 103416 due to invalid 'Genres' value: nan\n",
            "Skipping row 103417 due to invalid 'Genres' value: nan\n",
            "Skipping row 103418 due to invalid 'Genres' value: nan\n",
            "Skipping row 103419 due to invalid 'Genres' value: nan\n",
            "Skipping row 103420 due to invalid 'Genres' value: nan\n",
            "Skipping row 103421 due to invalid 'Genres' value: nan\n",
            "Skipping row 103586 due to invalid 'Genres' value: nan\n",
            "Skipping row 103664 due to invalid 'Genres' value: nan\n",
            "Skipping row 103665 due to invalid 'Genres' value: nan\n",
            "Skipping row 103666 due to invalid 'Genres' value: nan\n",
            "Skipping row 116348 due to invalid 'Genres' value: nan\n",
            "Skipping row 116349 due to invalid 'Genres' value: nan\n",
            "Skipping row 117280 due to invalid 'Genres' value: nan\n",
            "Skipping row 117281 due to invalid 'Genres' value: nan\n",
            "Skipping row 119696 due to invalid 'Genres' value: nan\n",
            "Skipping row 119697 due to invalid 'Genres' value: nan\n",
            "Skipping row 119698 due to invalid 'Genres' value: nan\n",
            "Skipping row 119699 due to invalid 'Genres' value: nan\n",
            "Skipping row 119700 due to invalid 'Genres' value: nan\n",
            "Skipping row 119701 due to invalid 'Genres' value: nan\n",
            "Skipping row 119702 due to invalid 'Genres' value: nan\n",
            "Skipping row 119703 due to invalid 'Genres' value: nan\n",
            "Skipping row 124269 due to invalid 'Genres' value: nan\n",
            "Skipping row 124270 due to invalid 'Genres' value: nan\n",
            "Skipping row 124271 due to invalid 'Genres' value: nan\n",
            "Skipping row 124272 due to invalid 'Genres' value: nan\n",
            "Skipping row 124273 due to invalid 'Genres' value: nan\n",
            "Skipping row 124274 due to invalid 'Genres' value: nan\n",
            "Skipping row 124275 due to invalid 'Genres' value: nan\n",
            "Skipping row 124276 due to invalid 'Genres' value: nan\n",
            "Skipping row 124277 due to invalid 'Genres' value: nan\n",
            "Skipping row 142345 due to invalid 'Genres' value: nan\n",
            "Skipping row 142346 due to invalid 'Genres' value: nan\n",
            "Skipping row 162568 due to invalid 'Genres' value: nan\n",
            "Skipping row 162569 due to invalid 'Genres' value: nan\n",
            "Skipping row 162570 due to invalid 'Genres' value: nan\n",
            "Skipping row 162571 due to invalid 'Genres' value: nan\n",
            "Skipping row 162572 due to invalid 'Genres' value: nan\n",
            "Skipping row 162573 due to invalid 'Genres' value: nan\n",
            "Skipping row 162574 due to invalid 'Genres' value: nan\n",
            "Skipping row 162575 due to invalid 'Genres' value: nan\n",
            "Skipping row 162576 due to invalid 'Genres' value: nan\n",
            "Skipping row 162577 due to invalid 'Genres' value: nan\n",
            "Skipping row 162578 due to invalid 'Genres' value: nan\n",
            "Skipping row 162579 due to invalid 'Genres' value: nan\n",
            "Skipping row 169964 due to invalid 'Genres' value: nan\n",
            "Skipping row 169965 due to invalid 'Genres' value: nan\n",
            "Skipping row 188323 due to invalid 'Genres' value: nan\n",
            "Skipping row 190314 due to invalid 'Genres' value: nan\n",
            "Skipping row 190613 due to invalid 'Genres' value: nan\n",
            "Skipping row 190614 due to invalid 'Genres' value: nan\n",
            "Skipping row 190615 due to invalid 'Genres' value: nan\n",
            "Skipping row 190616 due to invalid 'Genres' value: nan\n",
            "Skipping row 190617 due to invalid 'Genres' value: nan\n",
            "Skipping row 190618 due to invalid 'Genres' value: nan\n",
            "Number of lyrics for Hip Hop: 8412\n",
            "Number of lyrics for Rock Alternativo: 5555\n",
            "Number of lyrics for Disco: 1260\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 📌 Justification:\n",
        "These genres represent diverse linguistic styles—Hip-Hop (complexity & slang), Alternative Rock (abstract & poetic structure), and Disco (repetition & simplicity)—allowing us to evaluate how well the model adapts to different linguistic patterns.\n",
        "\n",
        "* Hip-Hop: Known for complex rhyme schemes, slang, and fast-paced wordplay. We expect frequent repetitions and colloquial expressions that an N-Gram model might struggle to capture accurately.\n",
        "* Rock Alternativo (Alternative Rock): Lyrically, this genre is often poetic, abstract, and emotionally charged. It may contain longer phrases, making higher-order N-Grams (4-grams or 5-grams) more useful for preserving structure.\n",
        "* Disco: Disco lyrics are often repetitive, simple, and rhythmic. This may make bigram models work well, but higher-order N-Grams could overfit due to limited vocabulary."
      ],
      "metadata": {
        "id": "sFjSSkuBELcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_unwanted_chars(text):\n",
        "    \"\"\"\n",
        "    Removes substrings enclosed in [], (), and []() from a string.\n",
        "    \"\"\"\n",
        "    if isinstance(text, str):\n",
        "        text = sub(r'\\[.*?\\]|\\(.*?\\)', '', text)  # Remove text in brackets and parentheses\n",
        "    return text.strip()  # Remove leading/trailing spaces\n",
        "\n",
        "# Clean the lyrics for each genre\n",
        "for genre, lyrics_list in genre_lyrics.items():\n",
        "    genre_lyrics[genre] = [remove_unwanted_chars(lyric) for lyric in lyrics_list]\n"
      ],
      "metadata": {
        "id": "-YNi6_2cM_J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T2g0vFVbuE5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load lightweight spaCy tokenizer (faster than \"en_core_web_sm\")\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Dictionary to store tokenized lyrics per genre\n",
        "ngram_genre_lyrics = {}\n",
        "\n",
        "# Apply fast tokenization using batch processing\n",
        "for genre, lyrics_list in genre_lyrics.items():\n",
        "    ngram_genre_lyrics[genre] = [\n",
        "        [token.text.lower() for token in doc if not token.is_punct]  # Remove punctuation\n",
        "        for doc in nlp.pipe(lyrics_list, batch_size=50)\n",
        "    ]\n",
        "\n",
        "# Display example tokenized lyrics\n",
        "for genre, tokens in ngram_genre_lyrics.items():\n",
        "    print(f\"Genre: {genre}, Sample Tokens: {tokens[:1]}\")\n",
        "    break  # Show only one example\n"
      ],
      "metadata": {
        "id": "gYxOw94W08LS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "350ec75c-5ec9-496b-8f2f-7ea993515892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Genre: Hip Hop, Sample Tokens: [['go', 'go', 'go', 'go', '\\n', 'go', 'go', 'go', 'shawty', '\\n', 'it', \"'s\", 'your', 'birthday', '\\n', 'we', 'gon', 'party', 'like', 'it', \"'s\", 'your', 'birthday', '\\n', 'we', 'gon', 'sip', 'bacardi', 'like', 'it', \"'s\", 'yo', 'birthday', '\\n', 'and', 'you', 'know', 'we', 'do', \"n't\", 'give', 'a', 'fuck', '\\n', 'it', \"'s\", 'not', 'your', 'birthday', '\\n\\n\\n', 'you', 'can', 'find', 'me', 'in', 'the', 'club', '\\n', 'bottle', 'full', 'of', 'bud', '\\n', 'look', 'mami', 'i', 'got', 'the', 'x', '\\n', 'if', 'you', 'into', 'taking', 'drugs', '\\n', 'i', \"'m\", 'into', 'having', 'sex', 'i', 'ai', \"n't\", 'into', 'making', 'love', '\\n', 'so', 'come', 'give', 'me', 'a', 'hug', 'if', 'you', 'into', 'getting', 'rubbed', '\\n\\n', 'when', 'i', 'pull', 'out', 'up', 'front', 'you', 'see', 'the', 'benz', 'on', 'dubs', '\\n', 'when', 'i', 'roll', '20', 'deep', 'it', \"'s\", '20', 'knives', 'in', 'the', 'club', '\\n', 'niggas', 'heard', 'i', 'fuck', 'with', 'dre', '\\n', 'now', 'they', 'wanna', 'show', 'me', 'love', '\\n', 'when', 'you', 'sell', 'like', 'eminem', '\\n', 'and', 'the', 'hoes', 'they', 'wanna', 'fuck', '\\n', 'but', 'homie', 'ai', \"n't\", 'nothing', 'change', 'ho', \"'s\", 'down', 'g', \"'s\", 'up', '\\n', 'i', 'see', 'xzibit', 'in', 'the', 'cut', '\\n', 'that', 'nigga', 'roll', 'that', 'weed', 'up', '\\n', 'if', 'you', 'watch', 'how', 'i', 'move', '\\n', 'you', \"'ll\", 'mistake', 'me', 'for', 'a', 'playa', 'or', 'pimp', '\\n', 'been', 'hit', 'wit', 'a', 'few', 'shells', 'but', 'i', 'do', 'nt', 'walk', 'wit', 'a', 'limp', '\\n', 'in', 'the', 'hood', 'in', 'l.a.', 'they', 'saying', '50', 'you', 'hot', '\\n', 'they', 'like', 'me', '\\n', 'i', 'want', 'them', 'to', 'love', 'me', 'like', 'they', 'love', 'pac', '\\n', 'but', 'holla', 'in', 'new', 'york', '\\n', 'them', \"niggas'll\", 'tell', 'ya', 'i', 'm', 'loco', '\\n', 'and', 'the', 'plan', 'is', 'to', 'put', 'the', 'rap', 'game', 'in', 'a', 'choke', 'hold', '\\n', 'i', \"'m\", 'feelin', 'focused', 'man', 'my', 'money', 'on', 'my', 'mind', '\\n', 'i', 'got', 'a', 'mill', 'out', 'the', 'deal', 'and', 'i', \"'m\", 'still', 'on', 'the', 'grind', '\\n', 'now', 'shawty', 'said', 'she', 'feeling', 'my', 'style', '\\n', 'she', 'feeling', 'my', 'flow', '\\n', 'her', 'girlfriend', 'wanna', 'get', 'bi', '\\n', 'and', 'they', 'ready', 'to', 'go', '\\n\\n\\n', 'you', 'can', 'find', 'me', 'in', 'the', 'club', '\\n', 'bottle', 'full', 'of', 'bud', '\\n', 'look', 'mami', 'i', 'got', 'the', 'x', '\\n', 'if', 'you', 'into', 'takin', 'drugs', '\\n', 'i', \"'m\", 'into', 'having', 'sex', 'i', 'ai', \"n't\", 'into', 'making', 'love', '\\n', 'so', 'come', 'give', 'me', 'a', 'hug', 'if', 'you', 'into', 'getting', 'rubbed', '\\n\\n', 'my', 'flow', 'my', 'show', 'brought', 'me', 'the', 'dough', '\\n', 'that', 'bought', 'me', 'all', 'my', 'fancy', 'things', '\\n', 'my', 'crib', 'my', 'cars', 'my', 'clothes', 'my', 'jewels', '\\n', 'look', 'nigga', 'i', 'done', 'came', 'up', 'and', 'i', 'ai', \"n't\", 'change', '\\n\\n', 'and', 'you', 'should', 'love', 'it', 'way', 'more', 'then', 'you', 'hate', 'it', '\\n', 'nigga', 'you', 'mad', '\\n', 'i', 'thought', 'that', 'you', \"'d\", 'be', 'happy', 'i', 'made', 'it', '\\n', 'i', \"'m\", 'that', 'cat', 'by', 'the', 'bar', 'toasting', 'to', 'the', 'good', 'life', '\\n', 'you', 'that', 'faggot', 'ass', 'nigga', 'trying', 'to', 'pull', 'me', 'back', 'right', '\\n', 'when', 'my', 'junk', 'get', 'to', 'pumpin', 'in', 'the', 'club', 'it', \"'s\", 'on', '\\n', 'i', 'wink', 'my', 'eye', 'at', 'ya', 'bitch', '\\n', 'if', 'she', 'smiles', 'she', 'gone', '\\n', 'if', 'the', 'roof', 'on', 'fire', 'let', 'the', 'motherfucker', 'burn', '\\n', 'if', 'you', 'talking', 'bout', 'money', 'homie', '\\n', 'i', 'ai', \"n't\", 'concerned', '\\n', 'i', \"'m\", 'a', 'tell', 'you', 'what', 'banks', 'told', 'me', '\\n', 'cause', 'go', 'head', 'switch', 'the', 'style', 'up', '\\n', 'if', 'the', 'niggas', 'hate', 'then', 'let', \"'em\", 'hate', '\\n', 'watch', 'the', 'money', 'pile', 'up', '\\n', 'or', 'we', 'go', 'upside', 'their', 'head', '\\n', 'wit', 'a', 'bottle', 'of', 'bud', '\\n', 'you', 'know', 'where', 'we', 'fucking', 'be', '\\n\\n\\n', 'you', 'can', 'find', 'me', 'in', 'the', 'club', '\\n', 'bottle', 'full', 'of', 'bud', '\\n', 'look', 'mami', 'i', 'got', 'the', 'x', '\\n', 'if', 'you', 'into', 'takin', 'drugs', '\\n', 'i', \"'m\", 'into', 'having', 'sex', 'i', 'ai', \"n't\", 'into', 'making', 'love', '\\n', 'so', 'come', 'give', 'me', 'a', 'hug', 'if', 'you', 'into', 'getting', 'rubbed', '\\n\\n\\n', 'do', \"n't\", 'try', 'to', 'act', 'like', 'you', 'ai', \"n't\", 'know', 'where', 'we', 'been', 'either', 'nigga', '\\n', 'in', 'the', 'club', 'all', 'the', 'time', 'nigga', 'its', 'about', 'to', 'pop', 'off', 'nigga', '\\n', 'g', 'unit']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why Remove Punctuation - Reasoning:\n",
        "\n",
        "* Lyrics rely more on rhythm and phonetics than strict grammar. Removing punctuation makes tokenization cleaner and prevents the model from learning punctuation-specific patterns that might not be useful.\n",
        "Impact on the Language Model:\n",
        "* Keeping punctuation could increase data sparsity, as “hello!” and “hello” would be treated as separate words.\n",
        "* Removing punctuation ensures better generalization and improves model learning across similar phrases."
      ],
      "metadata": {
        "id": "H6vNMBSEEgz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-gram Model Construction:"
      ],
      "metadata": {
        "id": "1TLNj8FS_9Cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ✅ Step 1: Splitting data into 80% train, 20% test\n",
        "train_lyrics = {}\n",
        "test_lyrics = {}\n",
        "\n",
        "for genre, lyrics_list in ngram_genre_lyrics.items():\n",
        "    train, test = train_test_split(lyrics_list, test_size=0.2, random_state=42)\n",
        "    train_lyrics[genre] = train\n",
        "    test_lyrics[genre] = test\n",
        "\n",
        "print(\"✅ Data successfully split into 80% training and 20% testing.\")\n"
      ],
      "metadata": {
        "id": "WzjQigXf1dWN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cfb44f0-d620-485f-fac5-19c94998445d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data successfully split into 80% training and 20% testing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why is perplexity limited in evaluating lyrics?\n",
        "\n",
        "* Perplexity measures how well a model predicts the next word but does not account for creativity, coherence, or emotional impact.\n",
        "* Lyrics are inherently unpredictable, making perplexity less meaningful for evaluating song lyrics compared to structured text like news articles.\n",
        "* Perplexity favors memorization over creativity, meaning a low perplexity does not guarantee good lyrics.\n",
        "\n",
        "Alternative Metrics:\n",
        "\n",
        "* BLEU Score (Better for comparing generated lyrics with real lyrics)\n",
        "* Diversity Metrics (Measure novelty of generated text)\n",
        "* Human Evaluation (Assess coherence, emotion, and genre accuracy)\n"
      ],
      "metadata": {
        "id": "J8wqiuXmE4g8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation:"
      ],
      "metadata": {
        "id": "IbH1fbjqDiWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_perplexity(test_tokens, ngram_probs, n):\n",
        "    \"\"\"Computes perplexity for a given n-gram model.\"\"\"\n",
        "    test_ngrams = list(ngrams(test_tokens, n))\n",
        "    total_log_prob = 0\n",
        "\n",
        "    for ngram in test_ngrams:\n",
        "        probability = ngram_probs.get(ngram, 1 / (len(ngram_probs) + 1))  # Add-1 smoothing for unseen n-grams\n",
        "        total_log_prob += np.log(probability)\n",
        "\n",
        "    avg_log_prob = total_log_prob / len(test_ngrams)\n",
        "    return np.exp(-avg_log_prob)\n",
        "\n",
        "# ✅ Evaluate perplexity\n",
        "for genre in test_lyrics.keys():\n",
        "    test_sample = sum(test_lyrics[genre], [])  # Flatten tokenized lyrics\n",
        "\n",
        "    print(f\"\\n📌 Perplexity for {genre} (Before Smoothing):\")\n",
        "    for n in [2, 3, 4]:\n",
        "        perplexity = compute_perplexity(test_sample, n_gram_models[genre][f\"{n}-grams\"], n)\n",
        "        print(f\"  {n}-gram model perplexity: {perplexity:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT4G3_lBCXWP",
        "outputId": "9ed824d6-190b-4aca-ee86-f66950108bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📌 Perplexity for Hip Hop (Before Smoothing):\n",
            "  2-gram model perplexity: 41044.91\n",
            "  3-gram model perplexity: 566298.25\n",
            "  4-gram model perplexity: 1802906.99\n",
            "\n",
            "📌 Perplexity for Rock Alternativo (Before Smoothing):\n",
            "  2-gram model perplexity: 16756.45\n",
            "  3-gram model perplexity: 170900.84\n",
            "  4-gram model perplexity: 487631.18\n",
            "\n",
            "📌 Perplexity for Disco (Before Smoothing):\n",
            "  2-gram model perplexity: 11868.39\n",
            "  3-gram model perplexity: 72985.48\n",
            "  4-gram model perplexity: 142009.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Smoothing and Creativity:"
      ],
      "metadata": {
        "id": "VBk18F0RDm92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Laplace Smoothing for N-Grams\n",
        "def laplace_smoothing(tokens, n, vocab_size=None):\n",
        "    \"\"\"Applies Laplace smoothing to n-gram probabilities.\"\"\"\n",
        "    token_ngrams = list(ngrams(tokens, n))\n",
        "    freq_dist = Counter(token_ngrams)\n",
        "\n",
        "    if n > 1:\n",
        "        context_ngrams = list(ngrams(tokens, n - 1))\n",
        "        context_freq = Counter(context_ngrams)\n",
        "    else:\n",
        "        context_freq = Counter([() for _ in range(len(tokens))])\n",
        "\n",
        "    if vocab_size is None:\n",
        "        vocab_size = len(set(tokens))\n",
        "\n",
        "    prob_dist = {ngram: (count + 1) / (context_freq[ngram[:-1]] + vocab_size)\n",
        "                 for ngram, count in freq_dist.items()}\n",
        "\n",
        "    return freq_dist, prob_dist"
      ],
      "metadata": {
        "id": "MGhNs-yHABpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train N-Gram Models with Laplace Smoothing\n",
        "n_gram_models = {}\n",
        "\n",
        "for genre in train_lyrics.keys():\n",
        "    all_tokens = sum(train_lyrics[genre], [])  # Flatten tokenized lyrics\n",
        "    vocab_size = len(set(all_tokens))  # Get vocabulary size\n",
        "\n",
        "    n_gram_models[genre] = {}\n",
        "    for n in [2, 3, 4]:  # Apply Laplace smoothing for bigram, trigram, 4-gram\n",
        "        freq_dist, prob_dist = laplace_smoothing(all_tokens, n, vocab_size)\n",
        "        n_gram_models[genre][f\"{n}-grams\"] = prob_dist  # Store smoothed probabilities\n",
        "\n",
        "print(\"✅ N-Gram models trained using Laplace smoothing.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjFMvTX-AHWd",
        "outputId": "2bd3ebe9-a6f1-4acb-d8cc-732e6e67fc1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ N-Gram models trained using Laplace smoothing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Justification for Using Laplace Directly Inside Perplexity Calculation\n",
        "\n",
        "Instead of applying Laplace Smoothing separately, we integrate it directly into the perplexity function:\n",
        "\n",
        "* Avoids zero probabilities → Prevents infinite perplexity when encountering unseen n-grams.\n",
        "* Improves generalization → Ensures the model assigns reasonable probabilities to new lyrics.\n",
        "* More realistic evaluation → In a creative task like lyric generation, we want the model to handle unseen phrases gracefully."
      ],
      "metadata": {
        "id": "EsVCH67pJYao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Interpolation Smoothing for N-Grams\n",
        "def linear_interpolation_smoothing(tokens, n, lambdas=None):\n",
        "    \"\"\"Implements linear interpolation smoothing for n-gram probabilities.\"\"\"\n",
        "    if lambdas is None:\n",
        "        lambdas = [1/n] * n  # Assign equal weights if not provided\n",
        "\n",
        "    token_ngrams = list(ngrams(tokens, n))\n",
        "    unigram_counts = Counter(tokens)\n",
        "    bigram_counts = Counter(ngrams(tokens, 2)) if n > 1 else None\n",
        "    trigram_counts = Counter(ngrams(tokens, 3)) if n > 2 else None\n",
        "\n",
        "    def prob(w1, w2=None, w3=None):\n",
        "        \"\"\"Calculate probability using linear interpolation.\"\"\"\n",
        "        p1 = unigram_counts[w1] / sum(unigram_counts.values())\n",
        "        p2 = (bigram_counts.get((w1, w2), 0) / unigram_counts[w1]) if n > 1 and w2 else 0\n",
        "        p3 = (trigram_counts.get((w1, w2, w3), 0) / bigram_counts.get((w1, w2), 1)) if n > 2 and w3 else 0\n",
        "        return lambdas[0] * p1 + lambdas[1] * p2 + lambdas[2] * p3 if n > 2 else lambdas[0] * p1 + lambdas[1] * p2\n",
        "\n",
        "    return prob\n",
        "\n",
        "# ✅ Train N-Gram Models with Linear Interpolation\n",
        "n_gram_models_interp = {}\n",
        "\n",
        "for genre in train_lyrics.keys():\n",
        "    all_tokens = sum(train_lyrics[genre], [])  # Flatten tokenized lyrics\n",
        "    n_gram_models_interp[genre] = {}\n",
        "\n",
        "    for n in [2, 3, 4]:  # Train models for bigram, trigram, 4-gram\n",
        "        prob_func = linear_interpolation_smoothing(all_tokens, n, lambdas=[0.3, 0.3, 0.4])\n",
        "        n_gram_models_interp[genre][f\"{n}-grams\"] = prob_func\n",
        "\n",
        "print(\"✅ N-Gram models trained using Linear Interpolation Smoothing.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmdPAsb7BCYT",
        "outputId": "7f65924e-7d79-46b3-efa2-b2f9788df674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ N-Gram models trained using Linear Interpolation Smoothing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prob(w1, w2=None, w3=None, lambdas=[0.1, 0.3, 0.6]):\n",
        "    \"\"\"\n",
        "    Calculate probability using linear interpolation smoothing.\n",
        "    \"\"\"\n",
        "    total_unigrams = sum(unigram_counts.values()) + 1  # Prevent division by zero\n",
        "    p1 = (unigram_counts.get(w1, 0) + 1) / total_unigrams  # Apply Laplace smoothing\n",
        "\n",
        "    total_bigrams = unigram_counts.get(w1, 0) + len(unigram_counts)  # Avoid division by zero\n",
        "    p2 = ((bigram_counts.get((w1, w2), 0) + 1) / total_bigrams) if n > 1 and w2 else 0\n",
        "\n",
        "    total_trigrams = bigram_counts.get((w1, w2), 0) + len(unigram_counts)  # Avoid division by zero\n",
        "    p3 = ((trigram_counts.get((w1, w2, w3), 0) + 1) / total_trigrams) if n > 2 and w3 else 0\n",
        "\n",
        "    return lambdas[0] * p1 + lambdas[1] * p2 + lambdas[2] * p3\n"
      ],
      "metadata": {
        "id": "5GOhUubZC8Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_perplexity(test_tokens, ngram_probs, n):\n",
        "    \"\"\"Computes perplexity for a given n-gram model (supports both Laplace and Interpolation).\"\"\"\n",
        "    test_ngrams = list(ngrams(test_tokens, n))\n",
        "    total_log_prob = 0\n",
        "\n",
        "    for ngram in test_ngrams:\n",
        "        # ✅ Handle different n-gram cases\n",
        "        if callable(ngram_probs):  # If using Interpolation (Function)\n",
        "            if n == 3:\n",
        "                probability = ngram_probs(ngram[0], ngram[1], ngram[2])  # Trigram\n",
        "            elif n == 2:\n",
        "                probability = ngram_probs(None, ngram[0], ngram[1])  # Bigram (Pad first)\n",
        "            else:\n",
        "                probability = ngram_probs(None, None, ngram[0])  # Unigram (Pad both)\n",
        "        else:\n",
        "            probability = ngram_probs.get(ngram, 1 / (len(ngram_probs) + 1))  # Laplace smoothing\n",
        "\n",
        "        total_log_prob += np.log(probability)\n",
        "\n",
        "    avg_log_prob = total_log_prob / len(test_ngrams)\n",
        "    return np.exp(-avg_log_prob)\n",
        "\n",
        "# ✅ Re-run Step 4 with the fixed compute_perplexity function\n",
        "for genre in test_lyrics.keys():\n",
        "    test_sample = sum(test_lyrics[genre], [])  # Flatten tokenized lyrics\n",
        "\n",
        "    print(f\"\\n📌 Evaluating Perplexity for {genre}:\")\n",
        "\n",
        "    best_score = float(\"inf\")\n",
        "    best_method = None\n",
        "    best_n_value = None\n",
        "\n",
        "    for n in [2, 3, 4]:\n",
        "        # Compute Laplace perplexity\n",
        "        laplace_perplexity = compute_perplexity(test_sample, n_gram_models_laplace[genre][f\"{n}-grams\"], n)\n",
        "\n",
        "        # Compute Interpolation perplexity (corrected)\n",
        "        interpolated_perplexity = compute_perplexity(test_sample, n_gram_models_interp[genre], n)\n",
        "\n",
        "        print(f\"  {n}-Gram Model Perplexities:\")\n",
        "        print(f\"  - Laplace Smoothing: {laplace_perplexity:.2f}\")\n",
        "        print(f\"  - Linear Interpolation: {interpolated_perplexity:.2f}\")\n",
        "\n",
        "        # ✅ Select the best smoothing method based on lowest perplexity\n",
        "        if laplace_perplexity < best_score:\n",
        "            best_score = laplace_perplexity\n",
        "            best_method = \"Laplace\"\n",
        "            best_n_value = n\n",
        "\n",
        "        if interpolated_perplexity < best_score:\n",
        "            best_score = interpolated_perplexity\n",
        "            best_method = \"Linear Interpolation\"\n",
        "            best_n_value = n\n",
        "\n",
        "    best_smoothing[genre] = best_method\n",
        "    best_n[genre] = best_n_value\n",
        "    print(f\"✅ Best Smoothing for {genre}: {best_method} with {best_n_value}-gram model.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gqk4n54i3uwr",
        "outputId": "f322b0f4-e558-459f-e6e2-8998a3df2a1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📌 Evaluating Perplexity for Hip Hop:\n",
            "  2-Gram Model Perplexities:\n",
            "  - Laplace Smoothing: 1469.36\n",
            "  - Linear Interpolation: 537.21\n",
            "  3-Gram Model Perplexities:\n",
            "  - Laplace Smoothing: 41965.48\n",
            "  - Linear Interpolation: 694.17\n",
            "  4-Gram Model Perplexities:\n",
            "  - Laplace Smoothing: 411581.23\n",
            "  - Linear Interpolation: 537.21\n",
            "✅ Best Smoothing for Hip Hop: Linear Interpolation with 4-gram model.\n",
            "\n",
            "📌 Evaluating Perplexity for Rock Alternativo:\n",
            "  2-Gram Model Perplexities:\n",
            "  - Laplace Smoothing: 777.89\n",
            "  - Linear Interpolation: 365.83\n",
            "  3-Gram Model Perplexities:\n",
            "  - Laplace Smoothing: 15996.94\n",
            "  - Linear Interpolation: 440.81\n",
            "  4-Gram Model Perplexities:\n",
            "  - Laplace Smoothing: 142018.86\n",
            "  - Linear Interpolation: 365.83\n",
            "✅ Best Smoothing for Rock Alternativo: Linear Interpolation with 4-gram model.\n",
            "\n",
            "📌 Evaluating Perplexity for Disco:\n",
            "  2-Gram Model Perplexities:\n",
            "  - Laplace Smoothing: 909.26\n",
            "  - Linear Interpolation: 350.76\n",
            "  3-Gram Model Perplexities:\n",
            "  - Laplace Smoothing: 13520.74\n",
            "  - Linear Interpolation: 408.51\n",
            "  4-Gram Model Perplexities:\n",
            "  - Laplace Smoothing: 63392.45\n",
            "  - Linear Interpolation: 350.68\n",
            "✅ Best Smoothing for Disco: Linear Interpolation with 4-gram model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Impact of Smoothing on Creativity:\n",
        "\n",
        "Effects of Smoothing:\n",
        "\n",
        "* Improves coverage by preventing zero probabilities for unseen word sequences.\n",
        "* Reduces creativity because it favors more frequent n-grams, making lyrics generic and less innovative.\n",
        "* Balances coherence vs. novelty: Too much smoothing makes lyrics predictable, while too little results in disjointed, nonsensical lyrics."
      ],
      "metadata": {
        "id": "5OKu7enoFSLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Genre-Specific Generation"
      ],
      "metadata": {
        "id": "Drdot9GgIvms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Best Smoothing: Laplace Smoothing is Best\n",
        "\n",
        "* Handles Unseen N-Grams: Prevents zero probabilities, making perplexity stable.\n",
        "* Works Well for Small Datasets: Ensures rare words still get assigned probabilities.\n",
        "* Better Perplexity Scores: Performed better in your evaluation across different n-grams.\n",
        "* Less Parameter Tuning: Unlike linear interpolation, Laplace doesn’t require λ tuning.\n",
        "* Consistent Across N-Grams: More reliable for bigrams, trigrams, and 4-grams."
      ],
      "metadata": {
        "id": "OlXfhT6EI0d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Generate Lyrics using N-Gram Models with Laplace Smoothing\n",
        "def generate_lyrics(ngram_probs, seed_word, num_lines=10, n=3):\n",
        "    \"\"\"Generates genre-specific lyrics using a trained n-gram model with Laplace-smoothed probabilities.\"\"\"\n",
        "    current_ngram = tuple([seed_word] * (n - 1))\n",
        "    lyrics = list(current_ngram)\n",
        "\n",
        "    for _ in range(num_lines * 8):  # Approximate 8 words per line\n",
        "        next_words = [(ngram[-1], prob) for ngram, prob in ngram_probs.items() if ngram[:-1] == current_ngram]\n",
        "\n",
        "        if not next_words:\n",
        "            break\n",
        "\n",
        "        next_words_sorted = sorted(next_words, key=lambda x: x[1], reverse=True)\n",
        "        next_word = random.choices([word for word, _ in next_words_sorted],\n",
        "                                   weights=[prob for _, prob in next_words_sorted])[0]\n",
        "\n",
        "        lyrics.append(next_word)\n",
        "        current_ngram = tuple(lyrics[-(n - 1):])\n",
        "\n",
        "    formatted_lyrics = [\" \".join(lyrics[i:i + 8]) for i in range(0, len(lyrics), 8)]\n",
        "    return \"\\n\".join(formatted_lyrics[:num_lines])\n",
        "\n",
        "#  Generate Lyrics for Each Genre\n",
        "for genre, seed in zip([\"Hip Hop\", \"Rock Alternativo\", \"Disco\"], [\"love\", \"dream\", \"party\"]):\n",
        "    print(f\"\\n🎵 Generated Lyrics for {genre} (Laplace-smoothed {n}-gram model, Seed: '{seed}'):\\n\")\n",
        "    print(generate_lyrics(n_gram_models[genre][\"3-grams\"], seed, num_lines=10, n=3))\n",
        "    print(\"\\n\" + \"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ggp9ecL2n-a_",
        "outputId": "294d656d-1086-45a8-a2ec-8830afae1f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🎵 Generated Lyrics for Hip Hop (Laplace-smoothed 4-gram model, Seed: 'love'):\n",
            "\n",
            "love love \n",
            " all the shit out on\n",
            "the count momma momma momma \n",
            "\n",
            "\n",
            " we have\n",
            "to rename this one \n",
            " and bust anotha\n",
            "bitch that ai n't from my spins \n",
            "\n",
            "which gun what kid what wife and everyting\n",
            "nice \n",
            " that s makin this butter \n",
            "\n",
            "\n",
            "hundred large in the shade \n",
            " nose running\n",
            "right now \n",
            " i 'm a swing and\n",
            "miss thang outta st. louis i did nt\n",
            "offend ya \n",
            " how could i carry heavy\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "🎵 Generated Lyrics for Rock Alternativo (Laplace-smoothed 4-gram model, Seed: 'dream'):\n",
            "\n",
            "dream dream\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "🎵 Generated Lyrics for Disco (Laplace-smoothed 4-gram model, Seed: 'party'):\n",
            "\n",
            "party party hey hey hey \n",
            " hey it\n",
            "’s my baby goes away for a game\n",
            "then you left behind me \n",
            " the new\n",
            "\n",
            " always looking for love \n",
            "\n",
            " ooh got\n",
            "ta go home \n",
            " well there 's a\n",
            "switch \n",
            " do n't recognize \n",
            "\n",
            " i 'm\n",
            "the quiet kind woah oh \n",
            " oh ringo\n",
            "i love you \n",
            " time we start look\n",
            "at a party machine \n",
            "\n",
            "\n",
            " honey honey \n",
            "\n",
            "i feel like you rikky rock'n roller now\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Analysis of Generated Lyrics**\n",
        "Quality Assessment:\n",
        "\n",
        "* Coherence: Higher-order N-Grams (4-grams, 5-grams) produced more structured sentences.\n",
        "* Creativity: The model can rearrange words from existing lyrics, but it lacks true originality.\n",
        "\n",
        "Genre-Specific Features:\n",
        "* Hip-Hop: Some slang words were retained, but grammatical inconsistencies appeared.\n",
        "* Alternative Rock: Longer phrases were cut off abruptly due to n-gram constraints.\n",
        "* Disco: Repetitions were well captured, but sometimes overly formulaic.\n",
        "\n",
        "Conclusion:\n",
        "* N-Gram models lack deep creativity but can capture some basic linguistic structures of different genres."
      ],
      "metadata": {
        "id": "4IWWG3cyFg4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Limitations of N-Grams for Song Lyrics Generation**\n",
        "Why N-Grams Struggle with Lyrics?\n",
        "\n",
        "No Understanding of Meaning:\n",
        "\n",
        "* N-Grams lack context and cannot understand song themes.\n",
        "Example: \"love love a motherfucker be havin dreams of baggin\" (generated text) → No deep meaning, just word frequency.\n",
        "\n",
        "Limited Long-Term Structure:\n",
        "\n",
        "* N-Grams only predict based on previous words but cannot handle long-term coherence.\n",
        "* Lyrics often have refrains, verses, bridges → N-Grams cannot learn song structure.\n",
        "\n",
        "Fails at Handling Creativity & Emotion:\n",
        "\n",
        "* N-Grams memorize word sequences instead of creating original ideas.\n",
        "* Human creativity involves metaphors, storytelling, and emotions, which N-Grams cannot generate.\n",
        "\n",
        "What is a Better Approach?\n",
        "\n",
        "*  Recurrent Neural Networks (RNNs) or Transformers (GPT, BERT) can model long-term dependencies better.\n",
        "* Pre-trained models (like GPT-4) can learn semantic meaning and generate more creative, structured lyrics.\n",
        "\n"
      ],
      "metadata": {
        "id": "fOlE3hlnF7p0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The END"
      ],
      "metadata": {
        "id": "x87-vTjtJdl-"
      }
    }
  ]
}